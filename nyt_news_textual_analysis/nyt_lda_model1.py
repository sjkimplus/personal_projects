# -*- coding: utf-8 -*-
"""NYT_LDA_Mode1l.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XvxazXQ8R-xttteniWk2iqrKUZs_-56x

Other Examples: https://medium.com/@adriensieg/text-similarities-da019229c894

## 1. TF-Hub universal encoder
"""

import tensorflow_hub as hub
import seaborn as sns
import numpy as np

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def inner_product(features):
    corr = np.inner(features[0], features[1])
    return corr

def plot_similarity(labels, features, rotation):
    corr = np.inner(features, features)
    # sns.set(font_scale=1.2)
    g = sns.heatmap(
        corr,
        xticklabels=labels,
        yticklabels=labels,
        vmin=0,
        vmax=1,
        cmap="YlOrRd")
    g.set_xticklabels(labels, rotation=rotation)
    g.set_title("Semantic Textual Similarity")

def run_and_plot(descriptions_, labels_):
    description_embeddings_ = embed(descriptions_)
    plot_similarity(labels_, description_embeddings_, 90)

"""## 2. Read and preprocess data"""

#-You SHOULD NOT run this cell unless you are running the notebook on Google Colab!
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

DIR = "/content/drive/My Drive/HCDS_Ass 3&4/data/NYT/"

df = pd.read_csv(open(DIR + 'NewYorkTimes1_updated.csv', errors='ignore'))

df

"""- Checked that there is no duplicate!
- 17 faculties do not have specified research areas
"""

df['headline'].duplicated().any()

"""## 3. Embed each research areas"""

tag_embeddings = embed(df['headline'])

print(df['headline'][0])
print(tag_embeddings[0])

tag_embeddings

df['tag_embeddings'] = list(tag_embeddings)
df.head()

# drop some unnecessary catogories
columns_to_drop = ['source', 'document_type', 'news_desk', 'section_name', 'word_count', '_id', 'abstract']
dff = df.drop(columns=columns_to_drop)
dff

# changing the time to a year only
import re

def extract_first_year(input_string):
    # Define a regular expression pattern
    pattern = r'20\d{2}'

    # Use re.search to find the first match in the string
    match = re.search(pattern, input_string)

    # Check if a match is found
    if match:
        # Extract the matched substring and return it
        return match.group(0)
    else:
        # Return None if no match is found
        return "None"

time = dff['pub_date']

for i in range(len(time)):
  time[i] = extract_first_year(time[i])

dff.rename(columns={'pub_date': 'Time'}, inplace=True)
dff.rename(columns={'headline': 'Title'}, inplace=True)
dff.head()

"""## 5. Visualize by network

### 5-1. Create edges

Create and visualize a network based on these edges
"""

import matplotlib.pyplot as plt
from itertools import combinations

# sort the data by time
df_sort = dff.sort_values('Time', ascending=True).reset_index(drop=True)
# create tuple
headline = list(combinations(df_sort['Title'],2))

df_sort.head()

#-Combinations without repetition = 53301
len(headline)

d_name_embed = pd.Series(df_sort['tag_embeddings'].values, index=df_sort['Title']).to_dict()
d_name_embed

# checking
len(d_name_embed)

l_i = []
l_j = []
l_w = []
for i in headline:
  l_i.append(i[0])
  l_j.append(i[1])
  l_w.append(inner_product((d_name_embed[i[0]],d_name_embed[i[1]])))

df_dyads = pd.DataFrame(
    {'node_i': l_i,
     'node_j': l_j,
     'weight': l_w
    })
df_dyads

df_dyads['weight'].describe()

sns.displot(df_dyads, x='weight', bins=30)

"""### **Topic modeling:**"""

dff

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import string
import re

def text_lowercase(text):
    return text.lower()

def remove_numbers(text):
    result = re.sub(r'\d+', '', text)
    return result

def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

def remove_whitespace(text):
    return  " ".join(text.strip().split())

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# remove stopwords function
def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    return filtered_text

def remove_short_words(l_token):
    return [word for word in l_token if len(word)>3]

from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer = PorterStemmer()

# stem words in the list of tokenised words
def stem_words(text):
    if isinstance(text, list):
        stems = [stemmer.stem(word) for word in text]
    else:
        word_tokens = word_tokenize(text)
        stems = [stemmer.stem(word) for word in word_tokens]
    return stems

# LDA modeling
!pip install pyLDAvis==2.1.2

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
from pprint import pprint

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# spacy for lemmatization
import spacy

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
# %matplotlib inline

# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

dff

# data = dff['Title']
# test = data.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
# test

#create 5 data variables each with the associated year
data13 = dff.loc[dff['Time'] == '2013', 'Title']
data14 = dff.loc[dff['Time'] == '2014', 'Title']
data15 = dff.loc[dff['Time'] == '2015', 'Title']
data16 = dff.loc[dff['Time'] == '2016', 'Title']
data17 = dff.loc[dff['Time'] == '2017', 'Title']
data18 = dff.loc[dff['Time'] == '2018', 'Title']

# checking
print("numbers:", len(data13), len(data14), len(data15), len(data16), len(data17), len(data18))
print("total:", len(data13) +len(data14)+ len(data15)+ len(data16)+ len(data17)+ len(data18))

# Text Normalization

import spacy

# Load the spaCy English model
nlp = spacy.load('en_core_web_sm')

# Function to filter out non-noun and non-verb words using spaCy
def filter_nouns_and_verbs(words):
    doc = nlp(" ".join(words))
    return [token.text for token in doc if token.pos_ in ['ADJECTIVE', 'VERB', 'ADVERB', 'NOUN']]

# apply normalization 1
norm_13 = data13.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()

# apply normalization 2: Filter out words except noun and verb
norm2_13 = [filter_nouns_and_verbs(word_list) for word_list in norm_13]

# apply this to the rest of the year datas
norm_14 = data14.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_14 = [filter_nouns_and_verbs(word_list) for word_list in norm_14]

norm_15 = data15.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_15 = [filter_nouns_and_verbs(word_list) for word_list in norm_15]

norm_16 = data16.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_16 = [filter_nouns_and_verbs(word_list) for word_list in norm_16]

norm_17 = data17.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_17 = [filter_nouns_and_verbs(word_list) for word_list in norm_17]

norm_18 = data18.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_18 = [filter_nouns_and_verbs(word_list) for word_list in norm_18]

# checking
print("number of title in yr15: ", len(norm2_15))
print("number of title in yr15: ", len(data15))

# Creating the dictionary
# Create Dictionary

from gensim import corpora, models

# make word to id and put in dictionary
id2word_custom13 = corpora.Dictionary(norm2_13)
id2word_custom14 = corpora.Dictionary(norm2_14)
id2word_custom15 = corpora.Dictionary(norm2_15)
id2word_custom16 = corpora.Dictionary(norm2_16)
id2word_custom17 = corpora.Dictionary(norm2_17)
id2word_custom18 = corpora.Dictionary(norm2_18)

# Filter words based on frequency (e.g., keep words that appear at least 5 times)
# id2word_custom13.filter_extremes(no_below=3)
# id2word_custom14.filter_extremes(no_below=3)
# id2word_custom15.filter_extremes(no_below=3)
# id2word_custom16.filter_extremes(no_below=3)
# id2word_custom17.filter_extremes(no_below=3)
# id2word_custom18.filter_extremes(no_below=3)

# Re-create the corpus with the filtered dictionary
# Term document frequency
corpus_custom13 = [id2word_custom13.doc2bow(text) for text in norm2_13]
corpus_custom14 = [id2word_custom14.doc2bow(text) for text in norm2_14]
corpus_custom15 = [id2word_custom15.doc2bow(text) for text in norm2_15]
corpus_custom16 = [id2word_custom16.doc2bow(text) for text in norm2_16]
corpus_custom17 = [id2word_custom17.doc2bow(text) for text in norm2_17]
corpus_custom18 = [id2word_custom18.doc2bow(text) for text in norm2_18]


# Viewing the first one for check
print(norm2_14[:10])
print(corpus_custom14[:10])

# checking
print("nb of words:",len(id2word_custom13),len(id2word_custom14),len(id2word_custom15),len(id2word_custom16),len(id2word_custom17),len(id2word_custom18))
print("nb of words total:",len(id2word_custom13)+len(id2word_custom14)+len(id2word_custom15)+len(id2word_custom16)+len(id2word_custom17)+len(id2word_custom18))

# This is the first model that I built using 10 topics.
# Build LDA model
lda_model_custom13 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom13,
                                                   id2word=id2word_custom13,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom14 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom14,
                                                   id2word=id2word_custom14,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom15 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom15,
                                                   id2word=id2word_custom15,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)
lda_model_custom16 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom16,
                                                   id2word=id2word_custom16,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom17 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom17,
                                                   id2word=id2word_custom17,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom18 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom18,
                                                   id2word=id2word_custom18,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

# pprint(lda_model_custom13.print_topics())
# print("-----------------------------------------------------------------------")
# pprint(lda_model_custom14.print_topics())
# print("-----------------------------------------------------------------------")
# pprint(lda_model_custom15.print_topics())
print("-----------------------------------------------------------------------")
pprint(lda_model_custom16.print_topics())
print("-----------------------------------------------------------------------")
pprint(lda_model_custom17.print_topics())
print("-----------------------------------------------------------------------")
# pprint(lda_model_custom18.print_topics())

# Visualize the topics
pyLDAvis.enable_notebook()

# vis_custom13 = pyLDAvis.gensim.prepare(lda_model_custom13, corpus_custom13, id2word_custom13, sort_topics=False)
# pyLDAvis.display(vis_custom13)

# vis_custom14 = pyLDAvis.gensim.prepare(lda_model_custom14, corpus_custom14, id2word_custom14, sort_topics=False)
# pyLDAvis.display(vis_custom14)

# vis_custom15 = pyLDAvis.gensim.prepare(lda_model_custom15, corpus_custom15, id2word_custom15, sort_topics=False)
# pyLDAvis.display(vis_custom15)

# vis_custom16 = pyLDAvis.gensim.prepare(lda_model_custom16, corpus_custom16, id2word_custom16, sort_topics=False)
# pyLDAvis.display(vis_custom16)

vis_custom17 = pyLDAvis.gensim.prepare(lda_model_custom17, corpus_custom17, id2word_custom17, sort_topics=False)
pyLDAvis.display(vis_custom17)

# vis_custom18 = pyLDAvis.gensim.prepare(lda_model_custom18, corpus_custom18, id2word_custom18, sort_topics=False)
# pyLDAvis.display(vis_custom18)