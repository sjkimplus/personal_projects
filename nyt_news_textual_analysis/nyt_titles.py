# -*- coding: utf-8 -*-
"""NYT_titles.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ingUzClMwffJrFJpW_KrRsCz4pthjOK8
"""

import os
import json
import time
import requests
import pandas as pd
from datetime import datetime

api_key = 'zTJxM1XvYxBsxEg0ISKpLe2TbgsQffM2' #Dea's key
query = 'lgbt'
begin_date = '20181120'
end_date = '20231119'

# Note the 'q=' indicates the query term, which in our case is 'music'.
# 'begin_date=' and 'end_date=' indicates the date range of interest.
base_url = "http://api.nytimes.com/svc/search/v2/articlesearch.json?q={}&begin_date={}&end_date={}&sort=relevance&api-key={}".format(query, begin_date, end_date, api_key)
base_url

#sending the api request
def send_request(base_url, p):
    '''Sends a request to the NYT Archive API for given date.'''
    url = base_url + "&page={}".format(p)
    r = requests.get(url).json()
    time.sleep(15) # bc NYTimes only allows access for 10 times per minute.
    return r

def parse_response(response):
    data = {
        'abstract': [],
        'web_url': [],
        'source': [],
        'headline': [],
        'pub_date': [],
        'document_type': [],
        'news_desk': [],
        'section_name': [],
        'word_count': [],
        '_id': []
    }

    if 'response' in response and 'docs' in response['response']:
        articles = response['response']['docs']
        for article in articles:
            data['abstract'].append(article.get('abstract', ''))
            data['web_url'].append(article.get('web_url', ''))
            data['source'].append(article.get('source', ''))
            data['headline'].append(article['headline'].get('main', ''))
            data['pub_date'].append(article.get('pub_date', ''))
            data['document_type'].append(article.get('document_type', ''))
            data['news_desk'].append(article.get('news_desk', ''))
            data['section_name'].append(article.get('section_name', ''))
            data['word_count'].append(article.get('word_count', 0))
            data['_id'].append(article.get('_id', ''))

    return pd.DataFrame(data)

tmp = send_request(base_url, 1)
tmp

print(tmp['response'])
for t in tmp['response']:
  print(t)
print(tmp['response']['docs'])
print(tmp['response']['meta'])

# replicates all of the procedures above.
def get_data(base_url):
  page = 1
  columns = ['abstract', 'web_url', 'source', 'headline', 'pub_date', 'document_type', 'news_desk', 'section_name', 'word_count', '_id']
  df = pd.DataFrame(columns=columns)
  while page <= 200:
      response = send_request(base_url, page)
      if 'response' in response: # if response exists
        if 'docs' in response['response'] and len(response['response']['docs']) > 0: # if docs exists
          tmp_df = parse_response(response)
          df = pd.concat([df, tmp_df])
          print('Collected & processed {} query page!'.format(page))
          page += 1
        else:
          print("docs don't exist")
          page += 1
      else:
        print("reponse failed!")
        page += 1

  print('Collected & processed metadata of {} articles!'.format(len(df)))
  return df

from google.colab import drive
drive.mount('/content/drive')

df_collected_articles = get_data(base_url)

# directory
DIR= "/content/drive/My Drive/HCDS_Ass 3&4/"
df = pd.DataFrame(df_collected_articles)
df.to_csv(DIR+'NewYorkTimes2.csv', index=False)
df.head()