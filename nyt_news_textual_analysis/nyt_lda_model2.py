# -*- coding: utf-8 -*-
"""NYT_LDA_Mode2l.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nAWFRr8UuOx6BCXjEgXbwOaECVvClQ6H

Other Examples: https://medium.com/@adriensieg/text-similarities-da019229c894

## 1. TF-Hub universal encoder
"""

import tensorflow_hub as hub
import seaborn as sns
import numpy as np

embed = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

def inner_product(features):
    corr = np.inner(features[0], features[1])
    return corr

def plot_similarity(labels, features, rotation):
    corr = np.inner(features, features)
    # sns.set(font_scale=1.2)
    g = sns.heatmap(
        corr,
        xticklabels=labels,
        yticklabels=labels,
        vmin=0,
        vmax=1,
        cmap="YlOrRd")
    g.set_xticklabels(labels, rotation=rotation)
    g.set_title("Semantic Textual Similarity")

def run_and_plot(descriptions_, labels_):
    description_embeddings_ = embed(descriptions_)
    plot_similarity(labels_, description_embeddings_, 90)

"""## 2. Read and preprocess data"""

#-You SHOULD NOT run this cell unless you are running the notebook on Google Colab!
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

DIR = "/content/drive/My Drive/HCDS_Ass 3&4/data/NYT/"

df = pd.read_csv(open(DIR + 'NewYorkTimes2_updated.csv', errors='ignore'))

df

df['headline'].duplicated().any()

# drop some unnecessary catogories
columns_to_drop = ['source', 'document_type', 'news_desk', 'section_name', 'word_count', '_id', 'abstract']
dff = df.drop(columns=columns_to_drop)
dff

# changing the time to a year only
import re

def extract_first_year(input_string):
    # Define a regular expression pattern
    pattern = r'20\d{2}'

    # Use re.search to find the first match in the string
    match = re.search(pattern, input_string)

    # Check if a match is found
    if match:
        # Extract the matched substring and return it
        return match.group(0)
    else:
        # Return None if no match is found
        return "None"

time = dff['pub_date']

for i in range(len(time)):
  time[i] = extract_first_year(time[i])

dff.rename(columns={'pub_date': 'Time'}, inplace=True)
dff.rename(columns={'headline': 'Title'}, inplace=True)
dff

"""### **LDA Topic modeling**"""

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import string
import re

def text_lowercase(text):
    return text.lower()

def remove_numbers(text):
    result = re.sub(r'\d+', '', text)
    return result

def remove_punctuation(text):
    translator = str.maketrans('', '', string.punctuation)
    return text.translate(translator)

def remove_whitespace(text):
    return  " ".join(text.strip().split())

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# remove stopwords function
def remove_stopwords(text):
    stop_words = set(stopwords.words("english"))
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word not in stop_words]
    return filtered_text

def remove_short_words(l_token):
    return [word for word in l_token if len(word)>3]

from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize
stemmer = PorterStemmer()

# stem words in the list of tokenised words
def stem_words(text):
    if isinstance(text, list):
        stems = [stemmer.stem(word) for word in text]
    else:
        word_tokens = word_tokenize(text)
        stems = [stemmer.stem(word) for word in word_tokens]
    return stems

# LDA modeling
!pip install pyLDAvis==2.1.2

# Commented out IPython magic to ensure Python compatibility.
import re
import numpy as np
import pandas as pd
from pprint import pprint

# Gensim
import gensim
import gensim.corpora as corpora
from gensim.utils import simple_preprocess
from gensim.models import CoherenceModel

# spacy for lemmatization
import spacy

# Plotting tools
import pyLDAvis
import pyLDAvis.gensim  # don't skip this
import matplotlib.pyplot as plt
# %matplotlib inline

# Enable logging for gensim - optional
import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)

import warnings
warnings.filterwarnings("ignore",category=DeprecationWarning)

dff

#create 5 data variables each with the associated year
data19 = dff.loc[dff['Time'] == '2019', 'Title']
data20 = dff.loc[dff['Time'] == '2020', 'Title']
data21 = dff.loc[dff['Time'] == '2021', 'Title']
data22 = dff.loc[dff['Time'] == '2022', 'Title']
data23 = dff.loc[dff['Time'] == '2023', 'Title']

# checking
print("numbers:", len(data19), len(data20), len(data21), len(data22), len(data23))
print("total:", len(data19)+ len(data20)+ len(data21)+ len(data22)+ len(data23))

# Text Normalization

import spacy

# Load the spaCy English model
nlp = spacy.load('en_core_web_sm')

# Function to filter out non-noun and non-verb words using spaCy
def filter_nouns_and_verbs(words):
    doc = nlp(" ".join(words))
    return [token.text for token in doc if token.pos_ in ['ADJECTIVE', 'VERB', 'ADVERB', 'NOUN']]

# apply normalization 1
norm_19 = data19.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()

# apply normalization 2: Filter out words except noun and verb
norm2_19 = [filter_nouns_and_verbs(word_list) for word_list in norm_19]

# apply this to the rest of the year datas
norm_20 = data20.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_20 = [filter_nouns_and_verbs(word_list) for word_list in norm_20]

norm_21 = data21.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_21 = [filter_nouns_and_verbs(word_list) for word_list in norm_21]

norm_22 = data22.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_22 = [filter_nouns_and_verbs(word_list) for word_list in norm_22]

norm_23 = data23.dropna().apply(text_lowercase).apply(remove_numbers).apply(remove_punctuation).apply(remove_whitespace).apply(remove_stopwords).apply(remove_short_words).apply(stem_words).values.tolist()
norm2_23 = [filter_nouns_and_verbs(word_list) for word_list in norm_23]

# checking
print("number of title in yr15: ", len(norm2_23))
print("number of title in yr15: ", len(data23))

# Creating the dictionary
# Create Dictionary

from gensim import corpora, models

# make word to id and put in dictionary
id2word_custom19 = corpora.Dictionary(norm2_19)
id2word_custom20 = corpora.Dictionary(norm2_20)
id2word_custom21 = corpora.Dictionary(norm2_21)
id2word_custom22 = corpora.Dictionary(norm2_22)
id2word_custom23 = corpora.Dictionary(norm2_23)

# Filter words based on frequency (e.g., keep words that appear at least 5 times)
# id2word_custom19.filter_extremes(no_below=3)
# id2word_custom20.filter_extremes(no_below=3)
# id2word_custom21.filter_extremes(no_below=3)
# id2word_custom22.filter_extremes(no_below=3)
# id2word_custom23.filter_extremes(no_below=3)

# Re-create the corpus with the filtered dictionary
# Term document frequency
corpus_custom19 = [id2word_custom19.doc2bow(text) for text in norm2_19]
corpus_custom20 = [id2word_custom20.doc2bow(text) for text in norm2_20]
corpus_custom21 = [id2word_custom21.doc2bow(text) for text in norm2_21]
corpus_custom22 = [id2word_custom22.doc2bow(text) for text in norm2_22]
corpus_custom23 = [id2word_custom23.doc2bow(text) for text in norm2_23]


# Viewing the first one for check
print(norm2_19[:10])
print(corpus_custom19[:10])

# checking
print("nb of words:",len(id2word_custom19),len(id2word_custom20),len(id2word_custom21),len(id2word_custom22),len(id2word_custom23))
print("nb of words total:",len(id2word_custom19)+len(id2word_custom20)+len(id2word_custom21)+len(id2word_custom22)+len(id2word_custom23))

# This is the first model that I built using 10 topics.
# Build LDA model

lda_model_custom19 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom19,
                                                   id2word=id2word_custom19,
                                                   num_topics=5,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom20 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom20,
                                                   id2word=id2word_custom20,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)
lda_model_custom21 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom21,
                                                   id2word=id2word_custom21,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom22 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom22,
                                                   id2word=id2word_custom22,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

lda_model_custom23 = gensim.models.ldamodel.LdaModel(corpus=corpus_custom23,
                                                   id2word=id2word_custom23,
                                                   num_topics=10,
                                                   random_state=100,
                                                   update_every=1,
                                                   chunksize=100,
                                                   passes=10,
                                                   alpha='auto',
                                                   per_word_topics=True)

pprint(lda_model_custom19.print_topics())
print("-----------------------------------------------------------------------")
# pprint(lda_model_custom20.print_topics())
# print("-----------------------------------------------------------------------")
# pprint(lda_model_custom21.print_topics())
print("-----------------------------------------------------------------------")
# pprint(lda_model_custom22.print_topics())
# print("-----------------------------------------------------------------------")
# pprint(lda_model_custom23.print_topics())

# Visualize the topics
pyLDAvis.enable_notebook()

vis_custom19 = pyLDAvis.gensim.prepare(lda_model_custom19, corpus_custom19, id2word_custom19, sort_topics=False)
pyLDAvis.display(vis_custom19)

# vis_custom20 = pyLDAvis.gensim.prepare(lda_model_custom20, corpus_custom20, id2word_custom20, sort_topics=False)
# pyLDAvis.display(vis_custom20)

# vis_custom21 = pyLDAvis.gensim.prepare(lda_model_custom21, corpus_custom21, id2word_custom21, sort_topics=False)
# pyLDAvis.display(vis_custom21)

# vis_custom22 = pyLDAvis.gensim.prepare(lda_model_custom22, corpus_custom22, id2word_custom22, sort_topics=False)
# pyLDAvis.display(vis_custom22)

# vis_custom23 = pyLDAvis.gensim.prepare(lda_model_custom23, corpus_custom23, id2word_custom23, sort_topics=False)
# pyLDAvis.display(vis_custom23)